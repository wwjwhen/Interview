这里面需要介绍一个非常重要的概念，代理损失函数，平时在神经网络或者是其他的机器学习任务中常见的损失函数就那么几个，主要原因还是因为他们可以保证自己是易于优化的。

举个例子就是线性分类器的标准的损失函数应该是0,1损失函数，也就是说位于分类超平面的一侧的时候为0，位于分类超平面的另外一侧的时候为1，但是这样的损失函数是不易于优化的，在参数空间中光滑性非常不好，因此，要想最优化0-1损失函数其实可以尝试去优化它的上界，只要他的上界是一个凸光滑函数，那么通过优化这个凸光滑函数，就可以达到优化0-1损失函数的目的了。

上述是代理损失函数的思想，下面要举几个例子了：比如大名鼎鼎的交叉熵损失函数（对数损失函数，逻辑斯蒂损失函数），以及hinge-loss损失函数。这两个损失函数分别对应着不同的分类器。我们知道分类的终极目标都是一致的，但是为了达到这个目标的途径是不同的，也就诞生了不同的优化目标，对应着不同的损失函数，比如交叉熵损失函数就对应着逻辑斯蒂回归，当然，神经网络也要用到这个损失函数。而对于hinge-loss而言，本质上就对应着正则化的线性支持向量机，这个是非常重要的概念了。也就是说不同的分类器具有不同的理念，比如说逻辑斯蒂回归只是想要最大化似然函数，线性支持向量机只是想要最大化分类间隔，分位数回归只是想要找到数据分布中的分位数边界，他们都有着不同的数学建模的形式，但是到最后都可以归结为对不同的损失函数的优化问题，而这些损失函数往往都是易于被优化的代理损失函数。